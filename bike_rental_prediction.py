# -*- coding: utf-8 -*-
"""running_python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/astg606/py_materials/blob/master/welcome/running_python.ipynb
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn import linear_model

# Reading data
bike_rentals=pd.read_csv("https://raw.githubusercontent.com/e-kirkland/datascience/master/Predicting%20Bike%20Rentals/bike_rental_hour.csv")
bike_rentals.head()

bike_rentals.info

bike_rentals.describe()

# Viewing a histogram of the count column

plt.hist(bike_rentals["cnt"])

# creating function for determining time of day
def assign_label(hour):
    if (hour >= 6) & (hour < 12):
        return 1
    elif (hour >= 12) & (hour < 18):
        return 2
    elif (hour >= 18) & (hour < 24):
        return 3
    elif (hour >= 0) & (hour < 6):
        return 4

# using assign_label to interpret each item in hour column
bike_rentals['time_label'] = bike_rentals['hr'].apply(assign_label)

"""Building Linear Regression Model"""

lr=linear_model.LinearRegression()

# def a function for training/testing
def train_and_test(df, k=0):
    # splitting dataframe from target column
    numeric_df = df.select_dtypes(include=['integer', 'float'])
    features = numeric_df.columns.drop('cnt')

    if k == 0:
        # splitting dataframe into train/test segments
        train = df.sample(frac=.8)
        test = df.loc[~df.index.isin(train.index)]
    
        lr.fit(train[features], train['cnt'])
        predictions = lr.predict(test[features])
        mse = mean_squared_error(test['cnt'], predictions)
        rmse = np.sqrt(mse) 
    return rmse

    #simple cross validation
    if k == 1:
        # Randomizing all rows from df
        shuffled_df = df.sample(frac=1, )
        # splitting dataframe into train/test segments
        train = df.sample(frac=.8)
        test = df.loc[~df.index.isin(train.index)]
        
        # training data
        lr.fit(train[features], train['cnt'])
        predictions_one = lr.predict(test[features])
        mse_one = mean_squared_error(test['cnt'], predictions_one)
        rmse_one = np.sqrt(mse_one)
        
        # test data
        lr.fit(test[features], test['cnt'])
        predictions_two = lr.predict(train[features])
        mse_two = mean_squared_error(train['cnt'], predictions_two)
        rmse_two = np.sqrt(mse_two)
        
        # averaging rmse
        avg_rmse = np.mean([rmse_one, rmse_two])
        print(rmse_one, rmse_two)
        return avg_rmse

"""Features to drop:

instant
casual (proxy for rental)
registered (proxy for rental)
dteday (redundant)
Features to dummy (categorical):

season
weathersit
"""

# Dropping unnecessary columns
dropcols = ['instant', 'dteday', 'casual', 'registered']
bike_rentals.drop(columns=dropcols, inplace=True)

# Getting dummies for others
seasondummies = pd.get_dummies(bike_rentals['season'],prefix='season')
weatherdummies = pd.get_dummies(bike_rentals['weathersit'], prefix='weathersit')
rentalsdf = pd.concat([bike_rentals, seasondummies, weatherdummies], axis=1)

# Dropping original dummy columns
dropcols = ['season', 'weathersit']
rentalsdf.drop(columns=dropcols, inplace=True)
rentalsdf.head()

# Holdout validation
train_and_test(rentalsdf, k=0)

# Importing Decision Tree model
from sklearn.tree import DecisionTreeRegressor

# splitting dataframe from target column
features = rentalsdf.columns.drop('cnt')

# Recreating train and test groups
train = rentalsdf.sample(frac=.8)
test = rentalsdf.loc[~rentalsdf.index.isin(train.index)]

# Fitting Decision Tree model
reg = DecisionTreeRegressor(min_samples_leaf=5)
reg.fit(train[features], train["cnt"])

DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,max_features=None, max_leaf_nodes=None,min_impurity_decrease=0.0, min_samples_leaf=5, min_samples_split=2,min_weight_fraction_leaf=0.0,random_state=None, splitter='best')

predictions = reg.predict(test[features])
np.sqrt(np.mean((predictions-test['cnt'])**2))

"""Decision Tree Results
The decision tree regressor appears to have reduced the error significantly, increasing the accuracy of the model.
"""

#Random Forest model

from sklearn.ensemble import RandomForestRegressor

# Fitting model to data
reg = RandomForestRegressor(min_samples_leaf=5)
reg.fit(train[features], train["cnt"])
RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',
                      max_depth=None, max_features='auto', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=5,min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=100, n_jobs=None, oob_score=False,
                      random_state=None, verbose=0, warm_start=False)

# Making predictions on test set
predictions = reg.predict(test[features])
np.sqrt(np.mean((predictions-test['cnt'])**2))

"""In the end, the three models delivered different levels of accuracy, as defined by RMSE:

Linear Regression (Simple cross validation): 130.7
Decision Tree: 55.0
Random Forest: 44.0
As a result, the Random Forest model has produced the most accurately-fitting model to predict bike rentals by hour.
"""